\documentclass[english,11pt,a4paper]{article}

\input{commands}

% ==============================================================================

%\usepackage{parskip}

\usepackage{a4wide}
\usepackage{tikz}
\usepackage{amsmath,epsfig,amssymb,amsbsy}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage[
	backend=biber,
	style=numeric,
	natbib=true,
	url=false, 
	doi=true,
	eprint=false
]{biblatex}

\addbibresource{references.bib}

\usepackage[
	a4paper, top=2.5cm, left=2.3cm, right=2.3cm
]{geometry}

\usepackage{enumitem}
\setlist[itemize]{noitemsep}

\usepackage[disable]{todonotes}

% ==============================================================================

\let\endtitlepage\relax
\endlinechar=-1

% ==============================================================================

\begin{document}

\begin{titlepage}
	\centering
	
	\includegraphics[width=0.25\textwidth]{images/Universitaet_Logo_RGB}\par
	\vspace{0.5cm}
	
	{\scshape \large Technical University of Munich \par}
	\vspace{0.5cm}
	
	{\bfseries \Large Optimization Algorithms for Deep Learning \par}
	{Interdisciplinary Project \par}
	\vspace{0.5cm}
	
	{Jens Gansloser \par}
	{Supervisor: Emanuel Laude \par}
	\vspace{0.5cm}
	
	{\today \par}
	\vspace{0.5cm}
\end{titlepage}

\section{Introduction}

Many complex tasks in engineering and science can be solved by learning a suitable model based on large training data sets with respect to some objective function. The state-of-the-art performing models often consist of deeply nested non-convex and non-smooth functions. With the increase in processing power it has recently become possible to train very deep models based on big datasets, often in a parallel manner. One example for these nested models are neural networks which are achieving high quality results in many application fields, often outperforming more classical methods. However, usually the training is done using stochastic gradient descent which is difficult to parallelize over layers and inhibits bad convergence properties. This work aims to compare different optimization algorithms for composite optimization to optimize deeply nested loss functions. In this setup, we largely ignore the nesting of deeper layers and model the neural network mapping as inner function $f$ of the composite mapping $E(u) = L(f(u))$. The required gradients for $f$ are calculated using the backpropagation algorithm. As a baseline for comparing the algorithms we will use stochastic gradient descent.

% Important for comparisons are convergence properties and stability as well as the possibility to parallelize the algorithms.

\section{Objective function}

In this work, the aim is to solve a supervised regression or classification problem involving a highly nested objective function. More specifically, a mapping $f(x)$ from the inputs $x$ to the corresponding outputs $y$ based on $N$ training samples $(x_n, y_n)$ should be learned. The problem can be written as

\begin{equation}
	\begin{aligned}
		& \underset{\{W_j\}}{\text{minimize}}
		& \left\{ E(\{W_j\}) = \sum_{i=1}^{N} L_i(f(\{W_j\};x_i);y_i) + R(\{W_j\})
		= L(f(\{W_j\};x);y) + R(\{W_j\}) \right\}
	\end{aligned}
	\label{eq:basis}
\end{equation}

with some suitable error measure $L_i(y_p;y)$, model parametrization $\{W_j\}$ and optional regularizer $R$. An example for an error measure is the squared l2 loss $L(y_p;y) = \frac{1}{2} \| y_p - y \|^2_2$ where $y_p$ is the model prediction and $y$ the ground truth data. The function $f(\{W_j\};x) = W_{K+1}h(W_Kh(\dots h(W_1x)))$ is a nested K-layer mapping as it is usual in neural networks where $W_j$ are the weight matrices $W_1,\dots,W_{K+1}$ which do not need to have the same size. To keep the formulation clear, the biases are integrated into the weight matrices. The nonlinear activation function $h(x)$ is applied element-wise and can also be non-differential. Examples for activation functions are the sigmoid function $h(x) = 1/(1 + e^{-x})$ or the ReLU $h(x) = \mathrm{max}(0, x)$. Note that the sum over all samples can also be omitted by reformulating the involved functions to act on all training data simultaneously as shown on the right-hand side of equation~\ref{eq:basis}. In the following, we stack all input samples into the matrix $x$, where each column is one sample point. Similarly, the target variables are stacked into the matrix $y$. The error function $L$ now operates on matrices. For simplicity, in the following we define $u = \{W_j\}$ and write $f(u)$ and $L(f(u))$, omitting the inputs $x$ and targets $y$. Additionally, we assume $R(u) = 0$, since the regularizer does not influence the comparisons of the different algorithms.

\section{Stochastic gradient descent}

The standard way of training neural networks is stochastic gradient descent. The gradient descent algorithm is used to find locally optimal parameters $u$. Here, in each step the gradient is calculated using the backpropagation algorithm and a fixed, randomly chosen number of samples from the training data. The network parameters are then updated using a fixed or diminishing stepsize. Note that in the case of stochastic gradient descent, using methods like Armijo linesearch does result in unstable behavior. For the convergence of this method the gradient needs to be Lipschitz continuous. The (deterministic) gradient descent scheme can be derived by approximating the objective function at each iteration using the convex majorizer

\begin{equation}
	E_{u^k}(u) = E(u^k) + \inner{\nabla E(u^k)}{u - u^k} + \frac{1}{2 \tau} \|u - u^k\|^2.
	\label{eq:gd_majorizer}
\end{equation}


While this method works often well in practice, it has only linear convergence and the tendency to get stuck in local minima, in comparison to more sophisticated non-convex majorization approaches. Additionally, finding a suitable step size often involves some heuristics and validation methods. It is well known that backpropagation leads to the vanishing gradient problem and therefore to small gradients which in turn requires the step size to be rather large, possibly leading to divergence of the algorithm.

\section{ADMM splitting}
\label{sec:last-layer_splitting}

The problems of gradient descent are tackled by using splitting based optimization algorithms \cites{carreira2014distributed}{taylor2016training} which show promising results with regard to stability and convergence rate. By splitting the composite function and introducing constraints we can reformulate problem~\ref{eq:basis} as

\begin{equation}
	\begin{aligned}
		& \underset{u,z}{\text{minimize}}
		&& L(z) \\
		& \text{subject to}
		&& z = f(u).
	\end{aligned}
	\label{eq:last-layer_splitting_problem}
\end{equation}

The augmented Lagrangian of this equality constrained problem can be written as

\begin{equation}
	\begin{aligned}
		\mathcal{L}(u, z, \lambda)
		&= L(z) + \inner{\lambda}{f(u)-z} + \frac{\rho}{2} \| f(u)-z \|^2 \\
		&= L(z) + \frac{\rho}{2} \| f(u) - z + \frac{1}{\rho} \lambda \|^2 - \frac{1}{2 \rho} \| \lambda \|^2.
	\end{aligned}
\end{equation}

Note that by adding a regularizer $R(u) = (1/2) \sum_{j} \| W_j \|^2_F$ for the weights we get the standard ADMM formulation. Applying dual ascent to problem~\ref{eq:last-layer_splitting_problem} yields

\begin{equation}
	\begin{aligned}
		u^{k+1} &:= \underset{u}{\text{arg min }} \mathcal{L}(u, z^k, \lambda^k) \\
		&:= \underset{u}{\text{arg min }} \frac{\rho}{2} \| f(u) - z^k + \frac{1}{\rho} \lambda^k \|^2 \\
	
		z^{k+1} &:= \underset{z}{\text{arg min }} \mathcal{L}(u^{k+1}, z, \lambda^k) \\
		&:= \underset{z}{\text{arg min }} L(z) + \frac{\rho}{2} \| f(u^{k+1}) - z + \frac{1}{\rho} \lambda^k \|^2 \\
		
		\lambda^{k+1} &:= \lambda^k + \rho (f(u^{k+1})-z^{k+1}).
	\end{aligned}
\end{equation}

The gradients of the two primal problems can be written as

\begin{equation}
	\begin{aligned}
		\nabla_{W_j} \mathcal{L} &= \rho \mathrm{J_f}(u)^T (f(u) - z^k + \frac{1}{\rho} \lambda^k) \\
		\nabla_z \mathcal{L} &= \nabla L(z) - \lambda^k - \rho (f(u^k) - z).
	\end{aligned}
\end{equation}

We denote the $u$ problem as primal 1, the $z$ problem as primal 2 and the $\lambda$ problem as dual. Convergence of ADMM in the non-convex setting is shown in \cite{hong2016convergence}. In our experiments, we solve primal 1 inexactly, for example by doing few gradient descent steps.

\subsection{Linearizing the outer function}

Interestingly, this formulation is similar to the minimization of a non-convex majorization function. Here, in each iteration the composite function $E(u) = L(f(u))$ is approximated by a non-convex majorizer shown in equation~\ref{eq:majorizer}.

\begin{equation}
	E_{u^k}(u) = L(f(u^k))
	+ \inner{\nabla L(f(u^k))}{f(u)-f(u^k)}
	+ \frac{1}{2 \tau} \|u - u^k \|^2
	\label{eq:majorizer}
\end{equation}

This majorizer is motivated by linearizing the outer function of the composite mapping. By assuming feasibility of the constraints of problem \ref{eq:last-layer_splitting_problem} $f(u) = z$, we get from the optimality condition of primal 2 $\nabla L(z) = \lambda^k$. Inserting this into the primal 1 update gives raise to the majorizer formulation in equation~\ref{eq:majorizer}. In \cite{geiping2018composite} the majorization formulation is generalized by using the Bregman distance function.

\subsection{Squared l2 loss}

Important to note is that in the case of the squared l2 loss, for a choice of $\rho=1$ the primal 1 update step is equivalent to the update step of the original unconstrained problem. Considering an iteration $k$, the first order necessary optimality condition for the primal 2 problem holds. This gives, by setting the gradient to zero, $z^{k+1} - \lambda^k - y = \rho(f(u^{k+1}) - z^{k+1})$. The dual can be written as $\lambda^{k+1} - \lambda^k = \rho(f(u)^{k+1} - z^{k+1})$. Therefore, in each iteration it holds that $y = z^{k+1} - \lambda^{k+1}$. Considering the next iteration $k+1$ and by setting $\rho=1$, the primal 1 problem can be written as 

\begin{equation}
	u^{k+1} = \underset{u}{\text{arg min }} \left\{ \mathcal{L}(u, z^k, \lambda^k)
	= \frac{1}{2} \|f(u) - y\|^2 = L(f(u)) \right\}.
\end{equation}

This holds for all iterations except for $k=1$.

\subsection{Batched primal updates}

The primal 1 problem can be minimized using various solvers for non-linear problems. However, in practice the training data is too large for a full batch approach. For example, in the case of a squared l2 loss using the Gauss-Newton method would include solving a system of linear equations involving a coefficient matrix $J^T J$ of size $Nc \times Nc$ with $c$ being the output dimension and $N$ the number of samples. Therefore, we use a batched approach where in each iteration the primal 1 update for the model parameters is done in a batched fashion. The primal 2 and dual updates are done full batch. In the case of the squared l2 loss both can be solved in closed form. The batched splitting algorithm is shown in algorithm~\ref{alg:sb}.

\begin{algorithm}
	\caption{Batched splitting}
	\label{alg:sb}
	\begin{algorithmic}[1]
		\State $x \gets$ Inputs
		\State $y \gets$ Targets
		\State Initialize $u^0, z^0, \lambda^0$
		\State Choose $\rho \in \R$
		\For{$k = 0,1,2,\dots$}
			\State $x_k, y_k \gets$ \Call{nextbatch}{$x, y, k$}
			\State $u^{k+1} \gets$ \Call{primal1}{$z^k, \lambda^k, x_k,y_k$} \Comment{Batched}
			\State $z^{k+1} \gets$ \Call{primal2}{$u^{k+1}, \lambda^k, x,y$} \Comment{Full batch}
			\State $\lambda^{k+1} \gets \lambda^k + \rho (f(u^{k+1};x)-z^{k+1})$ \Comment{Full batch}
		\EndFor
	\end{algorithmic}
\end{algorithm}

\section{Linearizing the inner function}

\todo{Majorizer?}

In comparison to equation~\ref{eq:majorizer}, it is also possible to linearize the inner function $f$ of the composite mapping $E(u) = L(f(u))$. With the additional penalty term on $u$, this can be written as

\begin{equation}
	E_{u^k}(u) = L(f(u^k) + J(u^k)(u-u^k)) + \frac{1}{2 \tau} \| u - u^k \|^2.	\label{eq:composite_inner_linearization}
\end{equation}

It is easy to see that this generalizes the Levenberg-Marquardt (LM) algorithm in the case of a squared error function $L$. Further properties and convergence results of this generalization can be found in \cite{lewis2016proximal}. In the following we focus on the LM algorithm. With the squared error loss $L(y_p;y) = \frac{1}{2} \|y_p - y\|^2$ equation~\ref{eq:composite_inner_linearization} can be written as

\begin{equation}
	E_{u^k}(u) = \frac{1}{2} \|f(u^k) + J(u^k)(u - u^k) - y\|^2 + \frac{1}{2 \tau} \| u - u^k \|^2.
	\label{eq:linearized_ls}
\end{equation}

The normal equations yield the LM update equations shown in equation~\ref{eq:LM_update}.

\begin{equation}
	(J^T J + \frac{1}{\tau} I) (u - u^k) = J^T(y - f(u^k))
	\label{eq:LM_update}
\end{equation}

To find a suitable damping parameter $\tau$, trust-region or line search methods can be used. Levenberg-Marquardt can only applied efficiently when the dimension of the mapping $f$ is relatively small. However, in our case we have large training data sets, which results in very large Jacobians. Similar to the splitting algorithm, we use a batched variant of the Levenberg-Marquardt algorithm where a randomly sampled subset of the training data is used to calculate the Jacobian.

\section{Experiments}

For all experiments, the pytorch deep learning library is used. It allows to efficiently compute the forward passes through the networks and to compute the network gradients using the backpropagation algorithm. The experiments are run on two different data sets. An artificial data set for binary classification where the network is trained with the cross-entropy loss and the MNIST data set where the least squares loss is used.

\subsection{Binary classification with cross-entropy loss}

In the case of $L$ being the negative log likelihood (also called cross-entropy) loss, it is difficult to solve the primal 2 problem efficiently, since a closed form solution does not exist. Additionally, it is not possible to solve this problem in a batched fashion. To keep the computational requirements low, we use a simple artificial data set for the experiments. We solve a simple, not linearly separable, binary classification problem. The training data set has input dimension two and consists of two interleaved noisy spirals, each spiral belonging to one class. The function $f$ is a three layer fully connected network with 12 hidden units in each layer. The ReLU is used as activation function. Good hyperparameters are empirically determined. For all experiments the same fixed step size is used. Figure \ref{fig:results_spirals} shows the loss over all samples with gradient descent (bGD) and ADMM splitting with a single gradient descent step as primal 1 update (sbGD). In figure \ref{fig:spirals_fullbatch} the results for the full batch version are shown, figure \ref{fig:spirals_batchsize10} shows the batched versions with batch size 10 of the respective algorithms.

\begin{figure}[t]
	\centering
	\begin{subfigure}[b]{0.5\textwidth}
		\includegraphics[width=\textwidth]{images/spirals_fullbatch.pdf}
		\caption{Full batch}
		\label{fig:spirals_fullbatch}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.5\textwidth}
		\includegraphics[width=\textwidth]{images/spirals_batchsize10.pdf}
		\caption{Batch size 10}
		\label{fig:spirals_batchsize10}
	\end{subfigure}
	
	\caption{Both plots show the value of the objective function over all samples at each iteration. The network is trained on an artificial binary classification data set using the cross-entropy loss. All experiments shown in figure \ref{fig:spirals_fullbatch} are run without batching. The experiments in figure \ref{fig:spirals_batchsize10} are run with a batch size of 10 samples. For sbGD10 we used $\rho=10$, for sbGD50 $\rho=50$ and for sbGD100 $\rho=100$.}
	
	\label{fig:results_spirals}
\end{figure}

\subsection{MNIST classification with least squares loss}

Additionally to the binary classification in the case of the cross-entropy loss, we run the experiments on the MNIST data set which is used to classify images of handwritten digits. Here, the error function $L$ is the least squares (also called squared l2) loss. The inner function $f$ is a simple three layer convolutional neural network. Again, the activation function is the ReLU and the hyperparameters are empirically determined. This classification problem can only be solved in reasonable time with batched training. We intentionally use a small batch size in the hope the ADMM splitting behaves more stable than stochastic gradient descent. For the comparisons, it is sufficient to train for one epoch. The stepsize is fixed and the same for all experiments. We compare batched gradient descent (bGD), batched ADMM splitting with a single gradient descent step as primal 1 update (sbGD) and batched Levenberg-Marquardt (bLM). Figure \ref{fig:mnist_batchsize10} shows the results with a batch size of 10 samples and varying $\rho$. For the experiments in figure \ref{fig:mnist_batchsize5} a batch size of 5 samples is used. Both plots show the value of the objective function over all samples at each iteration. In both cases, the convergence of ADMM splitting is worse than stochastic gradient descent. Each sbGD iteration is also much more costly as a simple gradient step. Additionally, there is no implicit regularization effect when using ADMM splitting.

\begin{figure}[t]
	\centering
	\begin{subfigure}[b]{0.5\textwidth}
		\includegraphics[width=\textwidth]{images/mnist_batchsize10.pdf}
		\caption{Batch size 10}
		\label{fig:mnist_batchsize10}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.5\textwidth}
		\includegraphics[width=\textwidth]{images/mnist_batchsize5.pdf}
		\caption{Batch size 5}
		\label{fig:mnist_batchsize5}
	\end{subfigure}

	\caption{Both plots show the value of the objective function over all samples at each iteration. The network is trained on the MNIST data set using the least squares loss. For figure \ref{fig:mnist_batchsize10} a batch size of 10 samples is used. For sbGD5 we used $\rho=5$, for sbGD10 $\rho=10$. The experiments in figure \ref{fig:mnist_batchsize5} are run with a batch size of 5 samples. For sbGD10 we set $\rho=10$, for sbGD50 $\rho=50$.}
	
	\label{fig:results_mnist}
\end{figure}

% ==============================================================================

\pagebreak

\printbibliography

\end{document}

