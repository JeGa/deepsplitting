\documentclass[english,11pt,a4paper]{article}

\input{macros}

\newcommand\inner[2]{\langle #1, #2 \rangle}

\usepackage{a4wide}
\usepackage{tikz}
%\usepackage{parskip}

\title{Ideas for Splitting Deep Networks}
\author{}
\usepackage{amsmath,epsfig,amssymb,amsbsy}

\begin{document}
\maketitle

\endlinechar=-1

\section{Objective function}

The aim is to solve a supervised regression or classification problem involving a highly nested objective function. The aim is to learn a mapping $f(x)$ from the inputs $x$ to the corresponding outputs $y$ based on $N$ training samples $(x_n, y_n)$. The loss function can be formulated as

\begin{equation}
E(\{W_j\}) = \frac{1}{N} \sum_{i=1}^{N} L(f(\{W_j\};x_i);y_i)
\label{eq:1}
\end{equation}

with some appropriate error measure $L(y_p;y)$ and model parametrization $\{W_j\}$. The error measure can for example be the squared l2-loss $L(y_p;y) = \frac{1}{2} \| y_p - y \|^2_2$ where $y_p$ are the model predictions and $y$ the ground truth data. The function $f(\{W_j\};x) = W_{K+1}h(W_Kh(\dots h(W_1x))$ is a nested K-layer mapping as it is usual in neural networks where $W_j$ are the weight matrices $W_1,\dots,W_{K+1}$ which do not need to have the same size. The nonlinear activation function $h(x)$ is applied element-wise and can also be non-differential. Examples are the sigmoid function $h(x) = 1/(1 + e^{-x})$ or the ReLU $h(x) = \mathrm{max}(0, x)$.

\section{Solution concept}

In the following, we stack all input samples into the matrix $x$ where each column is one sample point. Similarly, the target variables are stacked into the matrix $y$. The error function $L$ now operates on matrices. This formulation is equal to the formulation above, however, the layers now operate on all the data simultaneously. Additionally, without loss of generality we consider a 2-layer neural network mapping $f(\{W_j\};x) = W_3h(W_2h(W_1x))$ with a linear mapping to the output variables. The unconstrained optimization problem can then be written as

\begin{equation}
	\begin{aligned}
		& \underset{\{W_j\}}{\text{minimize}}
		& & L(W_3h(W_2h(W_1x);y).
	\end{aligned}
	\label{eq:loss_function}
\end{equation}

To split the nested objective function we decouple the weights and activation functions
by introducing additional variables and adding the appropriate constraints. In the following are different variants of decoupling the loss function. Note that we omit the constant $y$ in the following equations for the sake of simplicity.

\subsection{Idea 1: Per-layer decoupling}

By decoupling and introducing constraints we can reformulate problem \ref{eq:loss_function} as

\begin{equation}
	\begin{aligned}
		& \underset{\{W_j\},\{a_i\},z}{\text{minimize}}
		&& L(z) \\
		& \text{subject to}
		&&  a_1 = h(W_1x), \\
		&&& a_2 = h(W_2a_1), \\
		&&& z = W_3a_2
	\end{aligned}
\end{equation}

with the new variables $a_1, a_2, z$. The augmented Lagrangian of this equality constrained problem is

\begin{equation}
	\begin{aligned}
		\mathcal{L}(\{W_j\}, \{a_i\}, z, \{\lambda_j\}) = L(z) + 
		& \inner{\lambda_1}{h(W_1x)-a_1} + (\rho/2) \| h(W_1x)-a_1 \|^2 + \\
		& \inner{\lambda_2}{h(W_2a_1)-a_2} + (\rho/2) \| h(W_2a_1)-a_2 \|^2 + \\
		& \inner{\lambda_3}{W_3a_2-z} + (\rho/2) \| W_3a_2-z \|^2.
	\end{aligned}
\end{equation}

For minimizing the objective we update the weights and constraint variables of subproblems in an alternating fashion using gradient descent. In each update step we only consider the variables of one constraint and fix all other terms of the augmented Lagrangian. This results in one subproblem $\mathcal{H}_i$ for each constraint.

\begin{equation}
	\begin{aligned}
		\mathcal{H}_1(W_1,a_1,\lambda_1) &= && \inner{\lambda_1}{h(W_1x)-a_1} + (\rho/2) \| h(W_1x)-a_1 \|^2 + \\
			& && \inner{\lambda_2}{h(W_2a_1)-a_2} + (\rho/2) \| h(W_2a_1)-a_2 \|^2 \\
		\mathcal{H}_2(W_2,a_1,a_2,\lambda_2) &= && \inner{\lambda_2}{h(W_2a_1)-a_2} + (\rho/2) \| h(W_2a_1)-a_2 \|^2 + \\
			& && \inner{\lambda_1}{h(W_1x)-a_1} + (\rho/2) \| h(W_1x)-a_1 \|^2 + \\
			& && \inner{\lambda_3}{W_3a_2-z} + (\rho/2) \| W_3a_2-z \|^2 \\
		\mathcal{H}_3(W_3,a_2,z,\lambda_3) &= && \inner{\lambda_3}{W_3a_2-z} + (\rho/2) \| W_3a_2-z \|^2 + L(y,z) \\
			& && \inner{\lambda_2}{h(W_2a_1)-a_2} + (\rho/2) \| h(W_2a_1)-a_2 \|^2 
	\end{aligned}
\end{equation}

We need to take special care of the first subproblem (constraint for the first layer) involving the input $x$ and the last subproblem (constraint for the last linear layer) involving the predicted variable $z$. For all others we consider the update scheme (here for $\mathcal{H}_2$)

\begin{equation}
	\begin{aligned}
		W_2^{t+1} &= W_2^t - \rho_1 \nabla_{W_2}(\mathcal{H}_2(W_2^t,a_1^t,a_2^t,\lambda_2^t)) \\
		a_1^{t+1} &= a_1^t - \rho_2 \nabla_{a_1}(\mathcal{H}_2(W_2^t,a_1^t,a_2^t,\lambda_2^t)) \\
		a_2^{t+1} &= a_2^t - \rho_3 \nabla_{a_2}(\mathcal{H}_2(W_2^{t+1},a_1^{t+1},a_2^t,\lambda_2^t)) \\
		\lambda_2^{t+1} &= \lambda_2^{t} + \rho (h(W_2^{t+1}a_1^{t+1})-a_2^{t+1})
	\end{aligned}
\end{equation}

which are gradient descent steps for the primal variables and gradient ascent steps for the dual variable $\lambda$. For $\mathcal{H}_1$ only $W_1$, $a_2$ and $\lambda_1$ need to be updated since $x$ is a constant. For the last subproblem $\mathcal{H}_{K+1}$ we need to add $\nabla_zL(z)$ to the gradient with respect to $z$.

\subsection{Idea 2: Last-layer decoupling}

By decoupling and introducing constraints we can reformulate problem \ref{eq:loss_function} as

\begin{equation}
	\begin{aligned}
		& \underset{\{W_j\},z}{\text{minimize}}
		&& L(z) \\
		& \text{subject to}
		&& z = f(\{W_j\}).
	\end{aligned}
	\label{eq:llc_problem}
\end{equation}

The augmented Lagrangian of this equality constrained problem is

\begin{equation}
	\begin{aligned}
		\mathcal{L}(\{W_j\}, z, \lambda)
		&= L(z) + \inner{\lambda}{f(\{W_j\})-z} + (\rho/2)\| f(\{W_j\})-z \|^2 \\
		&= L(z) + (\rho/2) \| f(\{W_j\}) - z + (1/\rho) \lambda \|^2 - (1/(2 \rho)) \| \lambda \|^2.
	\end{aligned}
\end{equation}

Note that by adding a regularizer $R(\{W_j\}) = (1/2) \sum_{j} \| W_j \|^2$ for the weights we get the standard ADMM formulation. Applying dual ascent to problem \ref{eq:llc_problem} yields

\begin{equation}
	\begin{aligned}
		z^{k+1}
		&:= \underset{z}{\text{arg min }} \mathcal{L}(\{W_j\}^k, z, \lambda^k)
		= \underset{z}{\text{arg min }} L(z) + (\rho/2) \| f(\{W_j\}^k) - z + (1/\rho) \lambda^k \|^2 \\
		\{W_j\}^{k+1}
		&:= \underset{\{W_j\}}{\text{arg min }} \mathcal{L}(\{W_j\}, z^{k+1}, \lambda^k)
		= \underset{\{W_j\}}{\text{arg min }} (\rho/2) \| f(\{W_j\}) - z^{k+1} + (1/\rho) \lambda^k \|^2 \\
		\lambda^{k+1} &:= \lambda^k + \rho (f(\{W_j\}^{k+1})-z^{k+1}).
	\end{aligned}
\end{equation}

The gradients of the two primal problems can be written as

\begin{equation}
	\begin{aligned}
		\nabla_z \mathcal{L} &= \nabla L(z) - \lambda^k - \rho (f(\{W_j\}^k) - z) \\
		\nabla_{W_j} \mathcal{L} &= \rho J_f(\{W_j\})^T (f(\{W_j\}) - z^k + (1/\rho) \lambda^k).
	\end{aligned}
\end{equation}

\section{Appendix}

\subsection{Notation}

For a matrix $X$, $X_{ij}$ selects the element in row $i$ and column $j$. If only one index is present, depending on the subscript a matrix row or column is select. In this case, $X_i$ is row $i$, $X_j$ is column $j$.

\subsection{Energy gradients}

For the update steps we need the gradient of the terms $f(W,x,a,\lambda) = \inner{\lambda}{h(Wx)-a}$ and $g(W,x,a) = (1/2) \| h(Wx) - a \|^2$ with respect to $W \in \R^{m \times n}$, $x \in \R^{n \times p}$, $a \in \R^{m \times p}$ and $\lambda \in \R^{m \times p}$. The function $h$ is applied element-wise. Note that for the inner product the matrices are vectorized. \\ \\
For the gradients we need the following intermediate results. For applying the chain-rule the derivatives of $h(Wx) = h \in \R^{m \times p}$ w.r.t $W$ and $x$ are required. Taking the derivative for each element the entries for the Jacobian are $\partial_{W_i}h_{ij} = h'(W_i x_j) x_j^T$. Similarly, for the derivatives w.r.t. $x$ we get $\partial_{x_j}h_{ij} = h'(W_i x_j) W_i^T $. Note that we can easily get a formulation for all matrix elements at once by the fact that for the Jacobians it holds that $J_{(f \circ g)} = J_f \cdot J_g$ and row $i$ of $J_f$ is $\nabla f_i$. This results in

\begin{equation}
	\begin{aligned}
		\partial_{W_i} f(W,x,a,\lambda) &= \sum_{j=1}^{p} \lambda_{ij} h'(W_i x_j) x_j^T \\
		\partial_{x_j} f(W,x,a,\lambda) &= \sum_{i=1}^{m} \lambda_{ij} h'(W_i x_j) W_i^T \\
		\partial_{\lambda} f(W,x,a,\lambda) &= h(W x) - a \\
		\partial_{a} f(W,x,a,\lambda) &= -\lambda
	\end{aligned}
\end{equation}

for the gradients of $f$. For $g$ the gradients are

\begin{equation}
	\begin{aligned}
		\partial_{W_i} g(W,x,a) &= \sum_{j=1}^{p} (h(W_i x_j) - a_{ij}) h'(W_i x_j) x_j^T \\
		\partial_{x_j} g(W,x,a) &= \sum_{i=1}^{m} (h(W_i x_j) - a_{ij}) h'(W_i x_j) W_i \\		
		\partial_{a} g(W,x,a) &= a - h(Wx). \\
	\end{aligned}
\end{equation}

Note that the expressions are in a format where we get a short formula. For calculating the matrix derivatives for all rows or for the complete matrix at once some matrix stacking is required which would result in a longer expression.

For the multi-class classification experiments we use the softmax error function $\ell(x)_i = \exp(x_i)/(\sum_{j=1}^{n} \exp(x_j))$ where $x \in \R^n$. The derivative of the softmax function is $\partial_{x_k} \ell(x)_i = \ell(x)_i (\delta_{ik} - \ell(x)_k)$.

\subsection{Backpropagation}

As baseline we train the loss function using gradient descent. To calculate the gradients $\nabla_{W}E(W)$ of the loss function $E(W)$ backpropagation is used. Starting from the definition of the loss function in equation \ref{eq:1} we can calculate the gradient for each sample separately by computing $\nabla_{W}L(f(W;x_i);y_i)$ and summing over all samples. In the following we consider $W_{jk}^l$ as the weight connecting neuron $k$ to neuron $j$ of layer $l$. The linearities in layer $l$ are written as $z^l = W^la^{l-1} + b^l$ operating on the activations $a^{l-1}$. The activations of layer $l$ are computed as $a^l = h(W^l a^{l-1} + b^l) = h(z^l)$ with activation function $h$. Additionally, we have used the explicit formulation with the bias being a separate variable. This could also be included in the weight matrix by augmenting the activation vector with one and augmenting the weight matrix with the bias vector as last column. Figure \ref{fig:backprop} shows the neural network model at a layer $l$. We introduce the convention of $E^l_a$ being the error function of layer $l$ given the activations and $E^l_z$ being the error function of layer $l$ given the linearities. This allows us to split the error function and use the chain rule for computing the derivatives. Using this formulation, the goal is to calculate $\nabla_{W}E(W,b)$ and $\nabla_{b}E(W,b)$. By fixing all remaining weights and biases and applying the chain rule we get

\begin{equation}
	\begin{aligned}
		\frac{\partial E(W_{jk}^l)}{\partial W_{jk}^l} &= 
		\frac{\partial E^l_a(a^l(z^l(W_{jk}^l)))}{\partial W_{jk}^l} =
		\frac{\partial E^l_a}{\partial a^l} \frac{\partial a^l}{\partial z^l} \frac{\partial z^l}{\partial W^l_{jk}} =
		\frac{\partial E^l_a}{\partial a^l_j} \frac{\partial a^l_j}{\partial z^l_j} \frac{\partial z^l_j}{\partial W^l_{jk}}, \\
		\frac{\partial E(b_j^l)}{\partial b_j^l} &=
		\frac{\partial E^l_a(a^l(z^l(b_j^l)))}{\partial b_j^l} =
		\frac{\partial E^l_a}{\partial a^l} \frac{\partial a^l}{\partial z^l} \frac{\partial z^l}{\partial b_j^l} =
		\frac{\partial E^l_a}{\partial a^l_j} \frac{\partial a^l_j}{\partial z^l_j} \frac{\partial z^l_j}{\partial b_j^l}.
	\end{aligned}
	\label{eq:backprop_chainrule}
\end{equation}

Note that $\frac{\partial z^l_i}{\partial W^l_{jk}} = 0$ and $\frac{\partial z^l_i}{\partial b_j^l} = 0$ for all $i \neq j$ and $\frac{\partial a^l}{\partial z^l}$ is a diagonal matrix. We now introduce for each layer the error variable (row vector)

\begin{equation}
	\begin{aligned}
		\delta^l_j &= \frac{\partial E^l_a}{\partial a^l_j} \frac{\partial a^l_j}{\partial z^l_j} =
		\frac{\partial E^l_a}{\partial a^l_j} h'(z^l_j), \\
		\delta^l &= \frac{\partial E^l_a}{\partial a^l} \frac{\partial a^l}{\partial z^l} =
		\frac{\partial E^l_a}{\partial a^l} h'(z^l) = \frac{\partial E^l_a}{\partial a^l} \odot H'(z^l)^T
	\end{aligned}
\end{equation}

with $h'(z^l)$ being the Jacobian, $H'(z^l)$ the element-wise applied derivative of the activation function and $\odot$ the element-wise product. Note that the Jacobian is a diagonal matrix and we therefore can rewrite the equation using the Hadamard product. By expanding the first term we see that

\begin{equation}
	\begin{aligned}
		\frac{\partial E^l_a(a^l_j)}{\partial a^l_j} &= 
		\frac{\partial E^{l+1}_z(z^{l+1}(a^l_j))}{\partial a^l_j} = 
		\frac{\partial E^{l+1}_z}{\partial z^{l+1}} \frac{\partial z^{l+1}}{\partial a^l_j} = 
		\delta^{l+1}W^{l+1}_{:,j}, \\
		\frac{\partial E^l_a(a^l)}{\partial a^l} &= \delta^{l+1}W^{l+1}.
	\end{aligned}
\end{equation}

This shows that we can recursively compute the error variable $\delta^l$ starting from the last layer $L$ by

\begin{equation}
	\begin{aligned}
		\delta^l = \delta^{l+1}W^{l+1} h'(z^l) = \delta^{l+1}W^{l+1} \odot H'(z^l)^T.
	\end{aligned}
\end{equation}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.6\textwidth]{images/backprop}
	\caption{Model of the neural network loss function with weights $W$, biases $b$, linearities $z$ and activations $a$ displayed at some layer $l$.}
	\label{fig:backprop}
\end{figure}

What remains is to compute the last terms of the equations \ref{eq:backprop_chainrule}. Since $\frac{\partial z^l_j}{\partial W^l_{jk}} = a^{l-1}_k$ and $\frac{\partial z^l_j}{\partial b_j^l} = 1$ we get

\begin{equation}
	\begin{aligned}
		\frac{\partial E(W_{jk}^l)}{\partial W_{jk}^l} &= \delta^l_j a^{l-1}_k, \\
		\frac{\partial E(b_j^l)}{\partial b_j^l} &= \delta^l_j.
	\end{aligned}
\end{equation}

Special care needs to be taken at the last layer of the network. Since it is usual to have only a linear layer at the end of a network without activation function the error variable (omitting the constants in the loss function) becomes 

\begin{equation}
	\begin{aligned}
		\delta^L = \frac{\partial L(z^L)}{\partial z^L} = \nabla L^T.
	\end{aligned}
\end{equation}

To efficiently implement backpropagation we can directly derive a matrix formulation so the iteration over all sample points is not required. By reformulating the loss function as $L(f(W;X);Y)$ with $X \in \R^{d \times N}, Y \in \R^{c \times N}, L: (\R^{c \times N},\R^{c \times N}) \to \R$, sample dimension $d$ and output dimension $c$ the activations and linearities are now matrices. To calculate the derivatives the involved matrices are vectorized in column-major order. Working through the stated derivation again but with the vectorized matrices we can rewrite the expressions in matrix form as

\begin{equation}
	\begin{aligned}
		\delta^l_m &= \delta^{l+1}_m W^{l+1} h'(z^l) = \delta^{l+1}_m W^{l+1} \odot H'(z^l)^T, \\
		\delta^L_m &= \text{reshape}(\frac{\partial L(z^L)}{\partial z^L}) = L_m, \\
		D_W^l &= \text{reshape}(\delta^l J^l), \\
		D_b^l &= \sum_{i=1}^{N} (\delta^l_m)_{i}^T.
	\end{aligned}
\end{equation}

The gradients are represented by $\nabla_{W^l}E = D^l_W$ and $\nabla_{b^l}E = D^l_b$. Each row in the matrix $L_m$ is the transpose gradient w.r.t. each sample point $\nabla L(z^L_i)$. Each row of the matrix $H'$ is the derivative of the activation function applied element wise to the each sample point. For $D^l_b$ the summation is over the rows $i$ of the matrix $\delta^l_m$. Note that $D^l_W$ can be implemented faster using per element multiplication but is written in full matrix form for easier reading. The matrix $J^l$ is the Jacobian $\frac{\partial z^l}{\partial W^l}$ with $W^l$ in row-major or column-major vectorization. In this formulation the vector version of the error variable $\delta^l$ is used. The reshape function extracts the rows and columns accordingly to get the matrix form.

\subsection{Softmax}

The softmax function is often used as last layer in classification tasks. It enables to interpret the neural network outputs as probabilities. The softmax function $\phi: \R^d \to [0,1]^d$ and its Jacobian are defined as

\begin{equation}
	\begin{aligned}
		\phi(x)_i &= \frac{e^{x_i}}{\sum_{j=1}^{d} e^{x_j}} \\
		\phi'(x) &= \text{diag}(\phi(x)) - \phi(x) \phi(x)^T
	\end{aligned}
\end{equation}

where $\sum_{i=1}^{d}\phi(x)_i = 1$. We also can write the softmax function in matrix form where $\phi$ is applied to each column of $x \in \R^{d \times N}$. The resulting Jacobian is then a block-diagonal matrix.

%\subsection{Cross-Entropy loss}
%
%A very common loss function for classification is the cross-entropy loss (I wont state the explanation from information theory here). However, in the literature there are various derivations and formulations of this loss function. The cross entropy between two probability distributions $p$ and $q$ is defined as
%
%\begin{equation}
%	\begin{aligned}
%		H(p,q) = E_p[-\text{log}(q)].
%	\end{aligned}
%\end{equation}
%
%Starting from a general probabilistic viewpoint, lets define the likelihood of a family of models (indexed by the parameters $\theta$) given the data as $p(y|X;\theta)$ with $X$ being the input samples and $y$ the ground truth. In the maximum likelihood approach, we maximize this probability w.r.t. the parameters $\theta$ as shown in the following equation.
%
%\begin{equation}
%	\begin{aligned}
%		\theta_{ML} = && \underset{\theta}{\text{arg max}}
%		&& p(y|X;\theta)
%	\end{aligned}
%\end{equation}
%
%Which is equivalent to minimizing the negative log likelihood. Additionally assuming i.i.d samples this results in
%
%\begin{equation}
%\begin{aligned}
%L(y_t;y) = -\text{log}((y_t)_y)
%\end{aligned}
%\end{equation}
%
%\begin{equation}
%	p(y) = \prod_{i=1}^{c} \theta^{[y=i]}_i
%\end{equation}
%
%\begin{equation}
%	\begin{aligned}
%		L(y_t;y) = -\text{log}((y_t)_y)
%	\end{aligned}
%\end{equation}
%
%So maximum likelihood results in minimizing the cross entropy between empirical data distribution and unknown model distribution.
%
%The $y_t$ can be interpreted as probability distribution (from a softmax layer for example), $y$ is the true label.
%
%Multinomial.
%
%Keep in mind that there is also a different definition of cross entropy loss. In classification, it is also possible to interpret the likelihood for each label separately as Bernoulli distributed. TODO. This results in the summation of all labels with a per-label cross entropy loss
%
%\begin{equation}
%	\begin{aligned}
%		L(y_t;y) = -\sum_{i=1}^{c} y_i \text{log}(y_t)_i + (1 - y_i) \text{log}(1 - (y_t)_i)
%	\end{aligned}
%\end{equation}
%
%with $c$ the number of classes. In this case, the outputs do not have to form a probability distribution and can be for example from a sigmoid function. In the case of binary classification (only two classes) the two definitions are equal.

\section{Additional notes and to-dos}

\begin{itemize}
	\item Could add regularizer for the weights.
	\item Put in $L$ the groundtruth as constants.
\end{itemize}

{\small
  \bibliographystyle{ieee}
  \bibliography{references}
}

\end{document}

