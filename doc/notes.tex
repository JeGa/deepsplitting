\documentclass[english,11pt,a4paper]{article}

\input{commands}

% ==============================================================================

%\usepackage{parskip}

\usepackage{a4wide}
\usepackage{tikz}
\usepackage{amsmath,epsfig,amssymb,amsbsy}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage[
	backend=biber,
	style=numeric,
	natbib=true,
	url=false, 
	doi=true,
	eprint=false
]{biblatex}

\addbibresource{references.bib}

\usepackage[
	a4paper, top=2.5cm, left=2cm, right=2.5cm
]{geometry}

\usepackage{enumitem}
\setlist[itemize]{noitemsep}

\usepackage[]{todonotes}

% ==============================================================================

\let\endtitlepage\relax
\endlinechar=-1

% ==============================================================================

\begin{document}

\begin{titlepage}
	\centering
	
	\includegraphics[width=0.25\textwidth]{images/Universitaet_Logo_RGB}\par
	\vspace{0.5cm}
	
	{\scshape \large Technical University of Munich \par}
	\vspace{0.5cm}
	
	{\bfseries \Large Optimization Algorithms for Deep Learning \par}
	{Interdisciplinary Project \par}
	\vspace{0.5cm}
	
	{Jens Gansloser \par}
	{Supervisor: Emanuel Laude \par}
	\vspace{0.5cm}
	
	{\today \par}
	\vspace{0.5cm}
\end{titlepage}

\section{Introduction}

Many complex tasks in engineering and science can be solved by learning a suitable model based on large training data sets with respect to some objective function. The state-of-the-art performing models often consist of deeply nested non-convex and non-smooth functions. With the increase in processing power it has recently become possible to train very deep models based on big datasets, often in a parallel manner. One example for these nested models are neural networks which are achieving high quality results in many application fields, often outperforming more classical methods. However, usually the training is done using stochastic gradient descent which is difficult to parallelize over layers and inhibits bad convergence properties. This work aims to compare different optimization algorithms for composite optimization to optimize deeply nested loss functions. In this setup, we largely ignore the nesting of deeper layers and model the neural network mapping as inner function $f$ of the composite mapping $E(u) = L(f(u))$. The required gradients for $f$ are calculated using the backpropagation algorithm. As a baseline for comparing the algorithms we will use stochastic gradient descent.

% Important for comparisons are convergence properties and stability as well as the possibility to parallelize the algorithms.

\section{Objective Function}

In this work, the aim is to solve a supervised regression or classification problem involving a highly nested objective function. More specifically, a mapping $f(x)$ from the inputs $x$ to the corresponding outputs $y$ based on $N$ training samples $(x_n, y_n)$ should be learned. The problem can be written as

\begin{equation}
	\begin{aligned}
		& \underset{\{W_j\}}{\text{minimize}}
		& \left\{ E(\{W_j\}) = \sum_{i=1}^{N} L_i(f(\{W_j\};x_i);y_i) + R(\{W_j\})
		= L(f(\{W_j\};x);y) + R(\{W_j\}) \right\}
	\end{aligned}
	\label{eq:basis}
\end{equation}

with some suitable error measure $L_i(y_p;y)$, model parametrization $\{W_j\}$ and optional regularizer $R$. An example for an error measure is the squared l2 loss $L(y_p;y) = \frac{1}{2} \| y_p - y \|^2_2$ where $y_p$ is the model prediction and $y$ the ground truth data. The function $f(\{W_j\};x) = W_{K+1}h(W_Kh(\dots h(W_1x)))$ is a nested K-layer mapping as it is usual in neural networks where $W_j$ are the weight matrices $W_1,\dots,W_{K+1}$ which do not need to have the same size. To keep the formulation clear, the biases are incorporated into the weight matrices. The nonlinear activation function $h(x)$ is applied element-wise and can also be non-differential. Examples are the sigmoid function $h(x) = 1/(1 + e^{-x})$ or the ReLU $h(x) = \mathrm{max}(0, x)$. Note that the sum over all samples can also be omitted by reformulating the involved functions to act on all training data simultaneously as shown on the right-hand side in equation~\ref{eq:basis}. In the following, we stack all input samples into the matrix $x$ where each column is one sample point. Similarly, the target variables are stacked into the matrix $y$. The error function $L$ now operates on matrices. For simplicity, in the following we define $u = \{W_j\}$ and write $f(u)$ and $L(f(u))$, omitting the inputs $x$ and targets $y$. Additionally, we assume $R(u) = 0$, since the regularizer does not influence the comparisons of the different algorithms.

\section{Stochastic Gradient Descent}

The standard way of training neural networks is stochastic gradient descent. The gradient descent algorithm is used to find locally optimal parameters $u$. Here, in each step the gradient is calculated using the backpropagation algorithm and a fixed, randomly chosen number of samples from the training data. The network parameters are then updated using a fixed or diminishing stepsize. Note that in the case of stochastic gradient descent, using methods like Armijo linesearch does result in unstable behavior. For the convergence of this method the gradient needs to be lipschitz continuous. While this method works often well in practice, it has only linear convergence and the tendency to get stuck at local minima, in comparison to more sophisticated majorization approaches. Additionally, finding a suitable step size often involves some heuristics and validation methods. It is well known that backpropagation leads to the vanishing gradient problem and therefore to small gradients which in turn requires the step size to be rather large, possibly leading to divergence of the algorithm.

\section{ADMM Splitting}
\label{sec:last-layer_splitting}

The problems of gradient descent are tackled by using splitting based optimization algorithms \cites{carreira2014distributed,taylor2016training} which show promising results with regard to stability and convergence rate. By splitting the loss function at the network output and introducing constraints we can reformulate problem~\ref{eq:basis} as

\begin{equation}
	\begin{aligned}
		& \underset{\{W_j\},z}{\text{minimize}}
		&& L(z) \\
		& \text{subject to}
		&& z = f(\{W_j\}).
	\end{aligned}
	\label{eq:last-layer_splitting_problem}
\end{equation}

The augmented Lagrangian of this equality constrained problem is

\begin{equation}
	\begin{aligned}
		\mathcal{L}(\{W_j\}, z, \lambda)
		&= L(z) + \inner{\lambda}{f(\{W_j\})-z} + \frac{\rho}{2} \| f(\{W_j\})-z \|^2 \\
		&= L(z) + \frac{\rho}{2} \| f(\{W_j\}) - z + \frac{\lambda}{\rho} \|^2 - \frac{1}{2 \rho} \| \lambda \|^2.
	\end{aligned}
\end{equation}

Note that by adding a regularizer $R(\{W_j\}) = (1/2) \sum_{j} \| W_j \|^2$ for the weights we get the standard ADMM formulation. Applying dual ascent to problem~\ref{eq:last-layer_splitting_problem} yields

\begin{equation}
	\begin{aligned}
		\{W_j\}^{k+1} &:= \underset{\{W_j\}}{\text{arg min }} \mathcal{L}(\{W_j\}, z^k, \lambda^k) \\
		&:= \underset{\{W_j\}}{\text{arg min }} \frac{\rho}{2} \| f(\{W_j\}) - z^k + \frac{1}{\rho} \lambda^k \|^2 \\
	
		z^{k+1} &:= \underset{z}{\text{arg min }} \mathcal{L}(\{W_j\}^{k+1}, z, \lambda^k) \\
		&:= \underset{z}{\text{arg min }} L(z) + \frac{\rho}{2} \| f(\{W_j\}^{k+1}) - z + \frac{1}{\rho} \lambda^k \|^2 \\
		
		\lambda^{k+1} &:= \lambda^k + \rho (f(\{W_j\}^{k+1})-z^{k+1}).
	\end{aligned}
\end{equation}

The gradients of the two primal problems can be written as

\begin{equation}
	\begin{aligned}
		\nabla_{W_j} \mathcal{L} &= \rho \mathrm{J_f}(\{W_j\})^T (f(\{W_j\}) - z^k + \frac{1}{\rho} \lambda^k) \\
		\nabla_z \mathcal{L} &= \nabla L(z) - \lambda^k - \rho (f(\{W_j\}^k) - z).
	\end{aligned}
\end{equation}

We denote the $\{W_j\}$ problem as primal 1, the $z$ problem as primal 2 and the $\lambda$ problem as dual. Convergence of ADMM in the non-convex setting is shown in \cite{hong2016convergence}. In our experiments, we solve primal 1 inexactly, for example by doing few gradient descent steps.

\subsection{Majorization-Minimization}

Interestingly, this formulation is similar to the minimization of a non-convex majorizing function. Here, the composite function $E(x) = L(f(x))$ is approximated in each iteration by a non-convex majorizer shown in equation~\ref{eq:majorizer}.

\begin{equation}
	E_{u^k}(u) = L(f(u^k))
	+ \inner{\nabla L(f(u^k))}{f(u)-f(u^k)}
	+ R(u) + \frac{1}{2 \tau} \|u - u^k \|^2
	\label{eq:majorizer}
\end{equation}

This majorizer is motivated by linearizing the outer function of the composite mapping. By assuming feasibility of the constraints of problem \ref{eq:last-layer_splitting_problem} $f(\{W_j\}) = z$, we get from the optimality condition of primal 2 $\nabla = \lambda^k$. Inserting this in primal 1 update gives raise to the majorizer formulation in equation~\ref{eq:majorizer}. In \cite{geiping2018composite} the majorization formulation is generalized by using the Bregman distance function.

\subsection{l2 loss}

Important to note is that in the case of the squared l2 loss, for a choice of $\rho=1$ the algorithm is equivalent to gradient descent. Considering an iteration $k$, the first order necessary optimality condition for $z^{k+1} = \text{arg min } \mathcal{L}(\{W_j\}^{k+1}, z, \lambda^k)$ holds. This gives, by setting the gradient to zero, $z^{k+1} - \lambda^k - y = \rho(f(\{W_j\}^{k+1}) - z^{k+1})$. The dual can be written as $\lambda^{k+1} - \lambda^k = \rho(f(\{W_j\})^{k+1} - z^{k+1})$. Therefore, in each iteration it holds that $y = z^{k+1} - \lambda^{k+1}$. Considering the next iteration $k+1$ and by setting $\rho=1$, the primal 1 problem can be written as 

\begin{equation}
	\{W_j\}^{k+1} = \underset{\{W_j\}}{\text{arg min }} \mathcal{L}(\{W_j\}, z^k, \lambda^k)
	= \frac{1}{2} \|f(\{W_j\}) - y\|^2 = L(f(\{W_j\})).
\end{equation}

This holds for all iterations except for $k=1$.

\subsubsection{Batched Primal Updates}

The primal 1 problem can be minimized using various non-linear solvers. However, in practice the training data is too large for a full batch approach. For example, using Levenberg-Marquardt would include solving a system of linear equations involving a coefficient matrix $\mathrm{J}_f^T \mathrm{J}_f$ of size $Nd \times Nd$ with $d$ being the input data dimension. Therefore, we use a batched approach where in each iteration $k$ the primal update for the network weights is done in a batched fashion. The primal 2 and dual updates are done full batch. In the case of the squared l2 loss both can be solved in closed form. The batched splitting algorithm is shown in algorithm~\ref{alg:sb}.

\begin{algorithm}
	\caption{Batched last-layer splitting}
	\label{alg:sb}
	\begin{algorithmic}[1]
		\State $x \gets$ Inputs
		\State $y \gets$ Targets
		\State Initialize $\{W_j\}^0, z^0, \lambda^0$
		\State Choose $\rho \in \R$
		\For{$k = 0,1,2,\dots$}
			\State $x_k, y_k \gets$ \Call{nextbatch}{$x, y, k$}
			\State $\{W_j\}^{k+1} \gets$ \Call{primal1}{$x_k,y_k$}
			\State $z^{k+1} \gets$ \Call{primal2}{$x,y$}
			\State $\lambda^{k+1} \gets \lambda^k + \rho (f(\{W_j\}^{k+1};x)-z^{k+1})$
		\EndFor
	\end{algorithmic}
\end{algorithm}

\section{Levenberg-Marquardt}

Inner linearization.

The Levenberg-Marquardt algorithm can be used to iteratively solve the non-linear least squares problem shown in \ref{eq:basis_l2}. Here, in each iteration $f$ is linearly approximated by $f(\{W_j\}^k+\delta;x) \approx f(\{W_j\}^k;x) + \mathrm{J}\delta$ where $\mathrm{J}$ is the Jacobian at $\{W_j\}^k$. In each iteration the linearized problem shown in equation~\ref{eq:linearized_ls} is solved for $\delta$.

\begin{equation}
	E(\{W_j\}^k+\delta) \approx \frac{1}{2} \|f(\{W_j\}^k;x) + \mathrm{J}\delta - y\|^2
	\label{eq:linearized_ls}
\end{equation}

The normal equations yield

\begin{equation}
	(\mathrm{J}^T \mathrm{J}) \delta = \mathrm{J}^T(y - f(\{W_j\}^k;x)).
\end{equation}

By adding a damping term to the coefficient matrix on the left-hand side (which is equal to adding a regularizer term of the weights to the original problem) we get the Levenberg-Marquardt algorithm shown in equation~\ref{eq:levenberg-marquardt}.

\begin{equation}
	(\mathrm{J}^T \mathrm{J} + \lambda I) \delta = \mathrm{J}^T(y - f(\{W_j\}^k;x))
	\label{eq:levenberg-marquardt}
\end{equation}

The parameters are updated in each step according to $\{W_j\}^{k+1} = \{W_j\}^k + \delta$. To find a suitable damping parameter $\lambda$ trust-region or line search methods can be used.

\subsubsection{Batched Levenberg-Marquardt}

Levenberg-Marquardt can only applied efficiently when the dimension of the mapping $f$ is relatively small. However, in our case we have large training data sets, which results in very large Jacobians. Similar to the splitting algorithm used, we use a batched variant of the Levenberg-Marquardt algorithm where only a randomly sampled subset of the training data is used to calculate the Jacobian\todo{Convergence proof?}.

\section{Experiments}

For the experiments the well known MNIST data set is used. For efficient matrix calculus and utilization of GPUs the \textit{pytorch} library is used. Good hyperparameters for the respective algorithms are empirically determined.

% ==============================================================================

\pagebreak

\printbibliography

\end{document}

